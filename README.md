# kogpt2-chat-finetune
일상대화 데이터 전처리 및 파인튜닝(학습) 코드
- 데이터 출처 : https://aihub.or.kr/aihubdata/data/view.do?dataSetSn=543
- 데이터 구성 : 카카오톡, 페이스북, 인스타그램, 네이트온 다양한 SNS 플랫폼에서 수집된 원천 텍스트 데이터.
## Preprocessing
#### 불필요한 표현 정제
- 정규 표현식을 활용하여 한글, 영어, 숫자를 제외한 모든 특수문자 제거.
- 불용어 및 단일 감탄사 삭제: “아하”, “아이구”, “에이구” 등 의미가 없는 두 글자 이상의 감탄사와, “하”, “흠”, “음”과 같은 한 글자짜리 감탄사 제거.
- “하하하”, “키키키”, “흐흐흐”와 같이 의미 없이 반복된 감탄사들은 정규 표현식을 사용하여 모두 제거.
#### 데이터 필터링
- 파일 단위 제외 처리: 전처리된 문장 중 ‘*’ 문자가 하나라도 포함된 파일.('*'은 특정 인물의 이름이나 장소 등 마스킹되어 있는 텍스트)
#### 단어 표준화
- 영문 단어는 소문자로 변환하여 표준화.
#### 발화자별 문장 병합
- 대화 데이터에서 한 화자가 연속적으로 여러 번 말한 경우, 이를 자연스럽게 연결하여 하나의 문장으로 병합.<br><br>
원본:

![image](https://github.com/user-attachments/assets/ce1e4c8a-bcd5-4be1-b6a7-a297a8fb181a)

병합 후:


![Image](https://github.com/user-attachments/assets/d354113c-3761-41f9-9ee9-e672b9e2c023)

#### 입력-출력 데이터 쌍 생성
- 병합된 대화문을 기반으로, 화자 간 순차 대화 흐름을 따라 문장 페어 생성.
  - A가 먼저 말한 문장을 입력(input), B가 바로 응답한 문장을 출력(output)으로 구성.
  - 이어서 B의 응답을 input, A의 다음 응답을 output으로 한 번 더 구성.

  ![image](https://github.com/user-attachments/assets/528293d1-3b09-4e83-9068-119ae9175e9c)
- 학습용(Train) 데이터는 원본 텍스트 파일 87,690개에서 1,125,719개의 대화쌍을 생성.
- 검증용(Validation) 데이터는 원본 텍스트 파일 10,761개에서 162,262개의 대화쌍을 생성.
